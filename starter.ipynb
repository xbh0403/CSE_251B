{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "if \"tscc\" in current_dir:\n",
        "    os.chdir(\"/tscc/nfs/home/bax001/scratch/CSE_251B\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dU2bsn1w0bLs"
      },
      "source": [
        "# Use This if you are using Kaggle Notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2025-04-01T17:39:17.728787Z",
          "iopub.status.busy": "2025-04-01T17:39:17.728324Z",
          "iopub.status.idle": "2025-04-01T17:39:18.875979Z",
          "shell.execute_reply": "2025-04-01T17:39:18.874618Z"
        },
        "id": "3Wzm6DFQ0bLu",
        "papermill": {
          "duration": 1.154057,
          "end_time": "2025-04-01T17:39:18.878059",
          "exception": false,
          "start_time": "2025-04-01T17:39:17.724002",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "execution": {
          "iopub.execute_input": "2025-04-01T17:39:18.890746Z",
          "iopub.status.busy": "2025-04-01T17:39:18.890075Z",
          "iopub.status.idle": "2025-04-01T17:39:40.968717Z",
          "shell.execute_reply": "2025-04-01T17:39:40.967158Z"
        },
        "id": "xwTOeXGM0bLv",
        "outputId": "cc0b0245-24d5-4ec4-e7ae-1d5008e3692f",
        "papermill": {
          "duration": 22.084222,
          "end_time": "2025-04-01T17:39:40.970631",
          "exception": false,
          "start_time": "2025-04-01T17:39:18.886409",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_data's shape (10000, 50, 110, 6)\n",
            "test_data's shape (2100, 50, 50, 6)\n"
          ]
        }
      ],
      "source": [
        "# download the dataset to your folder or use it on kaggle notebook directly\n",
        "\n",
        "train_file = np.load('./data/train.npz')\n",
        "# train_file = np.load('/kaggle/input/cse-251-b-2025/train.npz')\n",
        "\n",
        "train_data = train_file['data']\n",
        "print(\"train_data's shape\", train_data.shape)\n",
        "test_file = np.load('./data/test_input.npz')\n",
        "# test_file = np.load('/Users/lilian/Documents/UCSD/CSE 251B/cse-251-b-2025/test_input.npz')\n",
        "\n",
        "test_data = test_file['data']\n",
        "print(\"test_data's shape\", test_data.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_scene(scene):\n",
        "    for i in range(scene.shape[0]):\n",
        "        xs = scene[i, :, 0]\n",
        "        ys = scene[i, :, 1]\n",
        "        xs = xs[xs != 0]\n",
        "        ys = ys[ys != 0]\n",
        "        plt.plot(xs, ys, alpha=0.5)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "def constant_velocity_baseline(data):\n",
        "    velocity = np.mean(data[...,1:,:2] - data[...,:-1,:2], axis=-2)\n",
        "    start = data[:, 0, -1, :2]\n",
        "    pred = np.stack([start + (i+1)*velocity for i in range(60)], axis=1)\n",
        "    return pred\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_features, output_features):\n",
        "        super(MLP, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(input_features, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(256, output_features)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        return self.mlp(x)\n",
        "\n",
        "def save_model(model, path=\"mlp_model.pth\"):\n",
        "    torch.save(model.state_dict(), path)\n",
        "    print(f\"Model saved to {path}\")\n",
        "\n",
        "def load_model(path=\"mlp_model.pth\", input_features=15000, output_features=120):\n",
        "    model = MLP(input_features, output_features)\n",
        "    model.load_state_dict(torch.load(path))\n",
        "    model.eval()\n",
        "    return model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(model, x_train, y_train, input_features, output_features, batch_size=64, epochs=10, lr=1e-3):\n",
        "    # Convert numpy arrays to PyTorch tensors\n",
        "    X_train_tensor = torch.FloatTensor(x_train).reshape((-1, input_features))\n",
        "    y_train_tensor = torch.tensor(y_train.reshape(-1, output_features), dtype=torch.float32)\n",
        "\n",
        "\n",
        "    # Create dataset and dataloader\n",
        "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Define loss and optimizer\n",
        "    criterion = nn.MSELoss()  # Or nn.CrossEntropyLoss() depending on the task\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        model.train()\n",
        "        for batch_X, batch_y in tqdm(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_X)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f'Epoch {epoch+1}, Loss: {running_loss / len(train_loader):.4f}')\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 157/157 [00:08<00:00, 19.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 775432.2075\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 157/157 [00:06<00:00, 24.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2, Loss: 337314.0837\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 157/157 [00:06<00:00, 25.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3, Loss: 246156.7949\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 157/157 [00:06<00:00, 25.93it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4, Loss: 260319.6008\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 157/157 [00:06<00:00, 25.86it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5, Loss: 261625.5249\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 157/157 [00:06<00:00, 25.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6, Loss: 194391.6296\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 157/157 [00:06<00:00, 26.07it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7, Loss: 183902.5359\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 157/157 [00:06<00:00, 25.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8, Loss: 207943.2407\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 157/157 [00:05<00:00, 26.25it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9, Loss: 194389.7673\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 157/157 [00:06<00:00, 25.79it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10, Loss: 180372.1986\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Restrict to 50 agents for training to match test data\n",
        "X_train = train_data[..., :50, :]           # shape: (10000, 50, 50, 6)\n",
        "y_train = train_data[:, 0, 50:, :2]           # shape: (10000, 60, 2)\n",
        "\n",
        "# Flatten for MLP\n",
        "X_flat = X_train.reshape(10000, -1)            # (10000, 15000)\n",
        "y_flat = y_train.reshape(10000, -1)            # (10000, 120)\n",
        "\n",
        "input_features = X_flat.shape[1]               # 15000\n",
        "output_features = y_flat.shape[1]              # 120\n",
        "# print( input_features, output_features)\n",
        "\n",
        "# Re-initialize and train new model\n",
        "model = MLP(input_features=input_features, output_features=output_features)\n",
        "trained_model = train_model(model, X_flat, y_flat, input_features, output_features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict(model, X_test, input_features):\n",
        "    \"\"\"Make predictions with the trained model.\"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        if isinstance(X_test, np.ndarray):\n",
        "            X_test_tensor = torch.FloatTensor(X_test)\n",
        "        else:\n",
        "            X_test_tensor = X_test.float()\n",
        "        X_test_tensor = X_test_tensor.view(-1, input_features)\n",
        "        predictions = model(X_test_tensor)\n",
        "    return predictions.numpy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Submission file saved as submission.csv\n"
          ]
        }
      ],
      "source": [
        "X_test = test_data.reshape(2100, -1)\n",
        "preds = predict(trained_model, X_test, input_features=15000)\n",
        "\n",
        "\n",
        "def save_submission(pred, filename):\n",
        "    reshaped = pred.reshape(-1, 2)\n",
        "    df = pd.DataFrame(reshaped, columns=['x', 'y'])\n",
        "    df.index.name = 'index'\n",
        "    df.to_csv(filename)\n",
        "\n",
        "# Save CSV\n",
        "import pandas as pd\n",
        "\n",
        "submission_file = 'submission.csv'\n",
        "save_submission(preds, submission_file)\n",
        "print(f\"Submission file saved as {submission_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved to mlp_model.pth\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "MLP(\n",
              "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "  (mlp): Sequential(\n",
              "    (0): Linear(in_features=15000, out_features=1024, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Dropout(p=0.1, inplace=False)\n",
              "    (3): Linear(in_features=1024, out_features=512, bias=True)\n",
              "    (4): ReLU()\n",
              "    (5): Dropout(p=0.1, inplace=False)\n",
              "    (6): Linear(in_features=512, out_features=256, bias=True)\n",
              "    (7): ReLU()\n",
              "    (8): Dropout(p=0.1, inplace=False)\n",
              "    (9): Linear(in_features=256, out_features=100, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "save_model(trained_model, \"mlp_model.pth\")\n",
        "load_model(\"mlp_model.pth\", input_features, output_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bV_YZci0bLy",
        "papermill": {
          "duration": 0.003051,
          "end_time": "2025-04-01T17:39:41.596387",
          "exception": false,
          "start_time": "2025-04-01T17:39:41.593336",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        " # Now you can submit to the leaderboard!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[main f405ba0] Updated training pipeline for CSE251B MLP\n",
            " 1 file changed, 10 insertions(+), 13 deletions(-)\n",
            "Enumerating objects: 5, done.\n",
            "Counting objects: 100% (5/5), done.\n",
            "Delta compression using up to 8 threads\n",
            "Compressing objects: 100% (3/3), done.\n",
            "Writing objects: 100% (3/3), 523 bytes | 523.00 KiB/s, done.\n",
            "Total 3 (delta 2), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (2/2), completed with 2 local objects.\u001b[K\n",
            "To github.com:LilianHeGao/CSE_251B.git\n",
            "   3afa6d0..f405ba0  main -> main\n"
          ]
        }
      ],
      "source": [
        "!git add .\n",
        "!git commit -m \"Updated training pipeline for CSE251B MLP\"\n",
        "!git push\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "name": "data-loading-and-submission-preperation",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30918,
      "isGpuEnabled": false,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "CSE251B",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 27.524611,
      "end_time": "2025-04-01T17:39:42.223757",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2025-04-01T17:39:14.699146",
      "version": "2.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
