{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dU2bsn1w0bLs"
      },
      "source": [
        "# Use This if you are using Kaggle Notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2025-04-01T17:39:17.728787Z",
          "iopub.status.busy": "2025-04-01T17:39:17.728324Z",
          "iopub.status.idle": "2025-04-01T17:39:18.875979Z",
          "shell.execute_reply": "2025-04-01T17:39:18.874618Z"
        },
        "id": "3Wzm6DFQ0bLu",
        "papermill": {
          "duration": 1.154057,
          "end_time": "2025-04-01T17:39:18.878059",
          "exception": false,
          "start_time": "2025-04-01T17:39:17.724002",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "execution": {
          "iopub.execute_input": "2025-04-01T17:39:18.890746Z",
          "iopub.status.busy": "2025-04-01T17:39:18.890075Z",
          "iopub.status.idle": "2025-04-01T17:39:40.968717Z",
          "shell.execute_reply": "2025-04-01T17:39:40.967158Z"
        },
        "id": "xwTOeXGM0bLv",
        "outputId": "cc0b0245-24d5-4ec4-e7ae-1d5008e3692f",
        "papermill": {
          "duration": 22.084222,
          "end_time": "2025-04-01T17:39:40.970631",
          "exception": false,
          "start_time": "2025-04-01T17:39:18.886409",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_data's shape (10000, 50, 110, 6)\n",
            "test_data's shape (2100, 50, 50, 6)\n"
          ]
        }
      ],
      "source": [
        "# download the dataset to your folder or use it on kaggle notebook directly\n",
        "\n",
        "train_file = np.load('/Users/lilian/Documents/UCSD/CSE 251B/cse-251-b-2025/train.npz')\n",
        "# train_file = np.load('/kaggle/input/cse-251-b-2025/train.npz')\n",
        "\n",
        "train_data = train_file['data']\n",
        "print(\"train_data's shape\", train_data.shape)\n",
        "test_file = np.load('/Users/lilian/Documents/UCSD/CSE 251B/cse-251-b-2025/test_input.npz')\n",
        "# test_file = np.load('/Users/lilian/Documents/UCSD/CSE 251B/cse-251-b-2025/test_input.npz')\n",
        "\n",
        "test_data = test_file['data']\n",
        "print(\"test_data's shape\", test_data.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torch'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mAgentTrajectoryDataset\u001b[39;00m(Dataset):\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class AgentTrajectoryDataset(Dataset):\n",
        "    def __init__(self, npz_file_path, split='train'):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            npz_file_path (str): Path to .npz file.\n",
        "            split (str): 'train' or 'test' to indicate which data to load.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        data = np.load(npz_file_path)\n",
        "        self.data = data['data']  # (N, 50, 110/50, 6)\n",
        "        self.split = split\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data[idx]  # (50, 110/50, 6)\n",
        "        \n",
        "        # Split position, velocity, heading, object_type separately\n",
        "        position = sample[..., 0:2]    # (50, T, 2)\n",
        "        velocity = sample[..., 2:4]    # (50, T, 2)\n",
        "        heading = sample[..., 4]       # (50, T)\n",
        "        object_type = sample[..., 5]   # (50, T)\n",
        "\n",
        "        return {\n",
        "            'position': torch.tensor(position, dtype=torch.float32),\n",
        "            'velocity': torch.tensor(velocity, dtype=torch.float32),\n",
        "            'heading': torch.tensor(heading, dtype=torch.float32),\n",
        "            'object_type': torch.tensor(object_type, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Example of using it:\n",
        "train_dataset = AgentTrajectoryDataset('/Users/lilian/Documents/UCSD/CSE 251B/cse-251-b-2025/train.npz')\n",
        "test_dataset = AgentTrajectoryDataset('/Users/lilian/Documents/UCSD/CSE 251B/cse-251-b-2025/test_input.npz')\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Peek one batch\n",
        "for batch in train_loader:\n",
        "    print(batch['position'].shape)  # (64, 50, 110, 2) for train\n",
        "    print(batch['object_type'].shape)  # (64, 50, 110)\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-01T17:39:40.97884Z",
          "iopub.status.busy": "2025-04-01T17:39:40.978362Z",
          "iopub.status.idle": "2025-04-01T17:39:41.323077Z",
          "shell.execute_reply": "2025-04-01T17:39:41.321787Z"
        },
        "id": "ZojFX-PX0bLv",
        "papermill": {
          "duration": 0.351374,
          "end_time": "2025-04-01T17:39:41.325147",
          "exception": false,
          "start_time": "2025-04-01T17:39:40.973773",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# plot one\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data_matrix = train_data[0]\n",
        "\n",
        "for i in range(data_matrix.shape[0]):\n",
        "    xs = data_matrix[i, :, 0]\n",
        "    ys = data_matrix[i, :, 1]\n",
        "    # trim all zeros\n",
        "    xs = xs[xs != 0]\n",
        "    ys = ys[ys != 0]\n",
        "    # plot each line going from transparent to full\n",
        "    plt.plot(xs, ys)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-01T17:39:41.339052Z",
          "iopub.status.busy": "2025-04-01T17:39:41.338689Z",
          "iopub.status.idle": "2025-04-01T17:39:41.348089Z",
          "shell.execute_reply": "2025-04-01T17:39:41.346792Z"
        },
        "id": "bJAcJRY-0bLw",
        "papermill": {
          "duration": 0.015255,
          "end_time": "2025-04-01T17:39:41.349879",
          "exception": false,
          "start_time": "2025-04-01T17:39:41.334624",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        " # say you have a model trained. we write a dummy model just to show useage\n",
        "\n",
        "def dummy_model(input_data):\n",
        "    return np.ones((2100, 1, 60, 2))\n",
        "\n",
        "\n",
        "output = dummy_model(test_data)\n",
        "output.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-01T17:39:41.358147Z",
          "iopub.status.busy": "2025-04-01T17:39:41.357784Z",
          "iopub.status.idle": "2025-04-01T17:39:41.587769Z",
          "shell.execute_reply": "2025-04-01T17:39:41.586758Z"
        },
        "id": "T_EYdfl80bLw",
        "papermill": {
          "duration": 0.236496,
          "end_time": "2025-04-01T17:39:41.589803",
          "exception": false,
          "start_time": "2025-04-01T17:39:41.353307",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# reshape to fit desired format: (2100, 1, 60, 2) -> (12600, 2)\n",
        "dummy_output = output.reshape(-1, 2)\n",
        "output_df = pd.DataFrame(dummy_output, columns=['x', 'y'])\n",
        "\n",
        "# adding a necessary step to match index of your prediction to that of the solution key\n",
        "\n",
        "output_df.index.name = 'index'\n",
        "\n",
        "output_df.to_csv('dummy_submission.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBHCCD-Z0bLw",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Split x and y for train data.\n",
        "\n",
        "train_x, train_y = train_data[..., :50, :], train_data[:, 0, 50:, :2]\n",
        "\n",
        "# get the average velocity of the prediction agent\n",
        "velocity_diff = train_x[...,1:, :2] - train_x[...,:-1, :2]\n",
        "print(velocity_diff.shape)\n",
        "\n",
        "constant_vel = np.mean(velocity_diff[:,0, :, :], axis=-2)\n",
        "print(constant_vel.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKq9hT-N0bLw",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# create pred_y\n",
        "\n",
        "pred_y = np.zeros((10000, 60, 2))\n",
        "starting_point = train_x[:, 0, -1, :2] # shape (10000, 2)\n",
        "\n",
        "for t in range(60):\n",
        "    pred_y[:,t,:] = starting_point + (t+1) * constant_vel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7eKqD4a0bLx",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# calculate train loss\n",
        "\n",
        "mse = ((train_y - pred_y)**2).mean()\n",
        "print(mse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufPZQTJ50bLx",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# prepare submission\n",
        "\n",
        "\n",
        "# get the average velocity of the prediction agent\n",
        "velocity_diff = test_data[...,1:, :2] - test_data[...,:-1, :2]\n",
        "print(velocity_diff.shape)\n",
        "\n",
        "constant_vel = np.mean(velocity_diff[:,0, :, :], axis=-2)\n",
        "print(constant_vel.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "643PMV4L0bLx",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# create pred_y for test set\n",
        "\n",
        "pred_y = np.zeros((2100, 60, 2))\n",
        "starting_point = test_data[:, 0, -1, :2]\n",
        "\n",
        "for t in range(60):\n",
        "    pred_y[:,t,:] = starting_point + (t+1) * constant_vel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKbXBYm50bLx",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# reshape to fit desired format: (2100, 60, 2) -> (12600, 2)\n",
        "pred_output = pred_y.reshape(-1, 2)\n",
        "output_df = pd.DataFrame(pred_output, columns=['x', 'y'])\n",
        "\n",
        "# adding a necessary step to match index of your prediction to that of the solution key\n",
        "\n",
        "output_df.index.name = 'index'\n",
        "\n",
        "output_df.to_csv('constant_vel_submission.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkiEM9Cz0bLx",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Split x and y for train data.\n",
        "\n",
        "train_x, train_y = train_data[..., :50, :], train_data[:, 0, 50:, :2]\n",
        "\n",
        "print(train_x.shape, train_y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgNBtoK90bLx",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_features, output_features):\n",
        "        super(MLP, self).__init__()\n",
        "\n",
        "        # Define the layers\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(input_features, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "\n",
        "            nn.Linear(256, output_features)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.mlp(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6_lSMHH0bLx",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Calculate the total number of features after flattening\n",
        "input_features = 50 * 50 * 6  # = 5000\n",
        "output_features = 60 * 2\n",
        "\n",
        "\n",
        "# Create the model\n",
        "model = MLP(input_features, output_features)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.MSELoss()  # For regression task\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4RzA0u-e0bLx",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "# Example of how to prepare data and train the model\n",
        "\n",
        "def train_model(model, x_train, y_train, batch_size=64, epochs=10):\n",
        "    # Convert numpy arrays to PyTorch tensors\n",
        "    X_train_tensor = torch.FloatTensor(x_train).reshape((-1, input_features))\n",
        "    y_train_tensor = torch.FloatTensor(y_train).reshape((-1, output_features))\n",
        "\n",
        "    # Create dataset and dataloader\n",
        "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for batch_X, batch_y in tqdm(train_loader):\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(batch_X)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(outputs, batch_y)\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Print epoch statistics\n",
        "        print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.4f}')\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IeA1HDPv0bLy",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "model = train_model(model, train_x, train_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4U58iotv0bLy",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "def predict(X_test):\n",
        "    \"\"\"Make predictions with the trained model\"\"\"\n",
        "    model.eval()  # Set to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        X_test_tensor = torch.FloatTensor(X_test).reshape((-1, input_features))\n",
        "        predictions = model(X_test_tensor).reshape((-1, 60, 2))\n",
        "    return predictions.numpy()\n",
        "\n",
        "# Save model\n",
        "def save_model(path=\"mlp_model.pth\"):\n",
        "    torch.save(model.state_dict(), path)\n",
        "    print(f\"Model saved to {path}\")\n",
        "\n",
        "# Load model\n",
        "def load_model(path=\"mlp_model.pth\"):\n",
        "    loaded_model = MLP()\n",
        "    loaded_model.load_state_dict(torch.load(path))\n",
        "    loaded_model.eval()\n",
        "    return loaded_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARm9C1SS0bLy",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "pred_y = predict(test_data)\n",
        "\n",
        "pred_output = pred_y.reshape(-1, 2)\n",
        "output_df = pd.DataFrame(pred_output, columns=['x', 'y'])\n",
        "\n",
        "# adding a necessary step to match index of your prediction to that of the solution key\n",
        "\n",
        "output_df.index.name = 'index'\n",
        "\n",
        "output_df.to_csv('mlp_baseline.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bV_YZci0bLy",
        "papermill": {
          "duration": 0.003051,
          "end_time": "2025-04-01T17:39:41.596387",
          "exception": false,
          "start_time": "2025-04-01T17:39:41.593336",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        " # Now you can submit to the leaderboard!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "name": "data-loading-and-submission-preperation",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30918,
      "isGpuEnabled": false,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 27.524611,
      "end_time": "2025-04-01T17:39:42.223757",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2025-04-01T17:39:14.699146",
      "version": "2.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
